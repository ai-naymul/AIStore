name: Daily Scraper + Tweet

on:
  schedule:
    # This will run at 10:30 PM UTC every day
    - cron: '30 22 * * *'
  workflow_dispatch:  # Allows manual triggering

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache Playwright browsers
      uses: actions/cache@v3
      with:
        path: ~/.cache/ms-playwright
        key: ${{ runner.os }}-playwright-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-playwright-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Only install Playwright browsers if not cached
        if [ ! -d ~/.cache/ms-playwright ]; then
          playwright install firefox
        fi
    
    - name: Run scraper
      env:
        PRODUCT_HUNT_URL: ${{ secrets.PRODUCT_HUNT_URL }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
        TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
        TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
        TWITTER_ACCESS_SECRET: ${{ secrets.TWITTER_ACCESS_SECRET }}
      run: python runner.py
